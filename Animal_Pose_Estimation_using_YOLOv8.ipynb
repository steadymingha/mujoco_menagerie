{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81e8e69c-b236-4d01-9f78-0f705b8c915a",
   "metadata": {
    "id": "81e8e69c-b236-4d01-9f78-0f705b8c915a"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation</h1>\n",
    "\n",
    "In this blog post, we will specifically deal with keypoints estimation of **dogs** and show you how to fine-tune the very popular **YOLOv8** pose models from Ultralytics.\n",
    "\n",
    "<img src = \"https://learnopencv.com/wp-content/uploads/2023/09/yolov8m-predictions.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e0ca9-3848-458a-9dc1-84fd04e73706",
   "metadata": {
    "id": "c97e0ca9-3848-458a-9dc1-84fd04e73706"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1 The Stanford Dogs Dataset](#1-The-Stanford-Dogs-Dataset)\n",
    "* [2 Download Image Data and Keypoint Metadata](#2-Download-Image-Data-and-Keypoint-Metadata)\n",
    "* [3 Create YOLO Train and Valid Directories](#3-Create-YOLO-Train-and-Valid-Directories)\n",
    "* [4 Data Visualization](#4-Data-Visualization)\n",
    "* [5 Configurations](#5-Configurations)\n",
    "* [6 Training](#6-Training)\n",
    "* [7 Evaluation](#7-Evaluation)\n",
    "* [8 Predictions](#8-Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa62b33c-73d8-43bb-a988-cf3eb33e4de6",
   "metadata": {
    "id": "fa62b33c-73d8-43bb-a988-cf3eb33e4de6",
    "ExecuteTime": {
     "end_time": "2024-12-03T16:09:16.546341Z",
     "start_time": "2024-12-03T16:09:14.428700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorboard ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2d691f3-1431-486a-aed7-fead57df9a3c",
   "metadata": {
    "id": "f2d691f3-1431-486a-aed7-fead57df9a3c",
    "ExecuteTime": {
     "end_time": "2024-12-04T08:56:29.208319Z",
     "start_time": "2024-12-04T08:56:27.445308Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "from shutil import copyfile\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import yaml\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1",
   "metadata": {
    "id": "8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1"
   },
   "source": [
    "## 1 The Stanford Dogs Dataset\n",
    "\n",
    "For our experiments, we will use the **Stanford Dataset**, which contains 120 breeds of dogs across **20,580** images. Besides, the dataset also contains the bounding box annotations for these images.\n",
    "\n",
    "However, the keypoint annotations need to be downloaded from the **StandfordExtra** dataset by filling up a **[google form](https://forms.gle/sRtbicgxsWvRtRmUA)**. The keypoint annotations are provided across **12,538** images for `20` keypoints of dog pose (`3` for each leg, `2` for each ear, `2` for the tail, nose, and jaw).\n",
    "\n",
    "<img src=\"https://learnopencv.com/wp-content/uploads/2023/09/animal-pose-estimation-dog-kpts.png\" width=700>\n",
    "\n",
    "The authors have also provided keypoint metadata in the form of a CSV file containing the animal pose name, the color coding for each keypoint, etc. It, however, contains the info across 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c55a89f-5d0f-412a-a46d-719ad4e9a485",
   "metadata": {
    "id": "2c55a89f-5d0f-412a-a46d-719ad4e9a485"
   },
   "source": [
    "## 2 Download Image Data and Keypoint Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4e95c-d726-4122-a328-243aacccb6c2",
   "metadata": {
    "id": "66c4e95c-d726-4122-a328-243aacccb6c2"
   },
   "source": [
    "The `download_and_unzip` utility downloads and extracts the **`images.tar`** file containing the images. Besides, we shall also download the **`keypoint_definitions.csv`** containing the keypoint metadata, such as the animal pose name, color coding for each keypoint, etc., across all 24  keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30414c02-3d41-4d1b-a8b4-5b98a8da6cce",
   "metadata": {
    "id": "30414c02-3d41-4d1b-a8b4-5b98a8da6cce",
    "ExecuteTime": {
     "end_time": "2024-12-04T08:56:42.710338Z",
     "start_time": "2024-12-04T08:56:42.700546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download and dataset.\n",
    "def download_and_unzip(url, save_path):\n",
    "\n",
    "    print(\"Downloading and extracting assets...\", end=\"\")\n",
    "    file = requests.get(url)\n",
    "    open(save_path, \"wb\").write(file.content)\n",
    "\n",
    "    try:\n",
    "        # Extract tarfile.\n",
    "        if save_path.endswith(\".tar\"):\n",
    "            with tarfile.open(save_path, \"r\") as tar:\n",
    "                tar.extractall(os.path.split(save_path)[0])\n",
    "\n",
    "        print(\"Done\")\n",
    "    except:\n",
    "        print(\"Invalid file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43929d-4d80-4c45-a7be-4a6685e99f3f",
   "metadata": {
    "id": "0c43929d-4d80-4c45-a7be-4a6685e99f3f"
   },
   "source": [
    "All the downloaded images are extracted to the Images  directory. It has the following directory structure:\n",
    "\n",
    "```python\n",
    "Images/\n",
    "├── n02085620-Chihuahua\n",
    "│   ├── n02085620_10074.jpg\n",
    "│   ├── n02085620_10131.jpg\n",
    "│   └── ...\n",
    "├── n02085782-Japanese_spaniel\n",
    "│   ├── n02085782_1039.jpg\n",
    "│   ├── n02085782_1058.jpg\n",
    "│   └── n02085782_962.jpg\n",
    "└── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ed3126-089f-44ab-8724-59bf86108480",
   "metadata": {
    "id": "c0ed3126-089f-44ab-8724-59bf86108480",
    "ExecuteTime": {
     "end_time": "2024-12-04T09:07:29.572887Z",
     "start_time": "2024-12-04T08:57:07.509158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting assets..."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Download if dataset does not exists.\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(IMAGES_DIR):\n\u001B[0;32m---> 10\u001B[0m     \u001B[43mdownload_and_unzip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mIMAGES_URL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIMAGES_TAR_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     os\u001B[38;5;241m.\u001B[39mremove(IMAGES_TAR_PATH)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(ANNS_METADATA):\n",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m, in \u001B[0;36mdownload_and_unzip\u001B[0;34m(url, save_path)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdownload_and_unzip\u001B[39m(url, save_path):\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDownloading and extracting assets...\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28mopen\u001B[39m(save_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mwrite(file\u001B[38;5;241m.\u001B[39mcontent)\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;66;03m# Extract tarfile.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/sessions.py:746\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    743\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[0;32m--> 746\u001B[0m     \u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontent\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/models.py:902\u001B[0m, in \u001B[0;36mResponse.content\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    901\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 902\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content \u001B[38;5;241m=\u001B[39m \u001B[38;5;124;43mb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    904\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_content_consumed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    905\u001B[0m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[1;32m    906\u001B[0m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/requests/models.py:820\u001B[0m, in \u001B[0;36mResponse.iter_content.<locals>.generate\u001B[0;34m()\u001B[0m\n\u001B[1;32m    818\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    819\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 820\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw\u001B[38;5;241m.\u001B[39mstream(chunk_size, decode_content\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    821\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    822\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/urllib3/response.py:1060\u001B[0m, in \u001B[0;36mHTTPResponse.stream\u001B[0;34m(self, amt, decode_content)\u001B[0m\n\u001B[1;32m   1058\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1059\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 1060\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m data:\n\u001B[1;32m   1063\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m data\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/urllib3/response.py:949\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt, decode_content, cache_content)\u001B[0m\n\u001B[1;32m    946\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m amt:\n\u001B[1;32m    947\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer\u001B[38;5;241m.\u001B[39mget(amt)\n\u001B[0;32m--> 949\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raw_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    951\u001B[0m flush_decoder \u001B[38;5;241m=\u001B[39m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m (amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data)\n\u001B[1;32m    953\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decoded_buffer) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/urllib3/response.py:873\u001B[0m, in \u001B[0;36mHTTPResponse._raw_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    870\u001B[0m fp_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_error_catcher():\n\u001B[0;32m--> 873\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mread1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread1\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fp_closed \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    874\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n\u001B[1;32m    875\u001B[0m         \u001B[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001B[39;00m\n\u001B[1;32m    876\u001B[0m         \u001B[38;5;66;03m# Close the connection when no data is returned\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# not properly close the connection in all cases. There is\u001B[39;00m\n\u001B[1;32m    882\u001B[0m         \u001B[38;5;66;03m# no harm in redundantly calling close.\u001B[39;00m\n\u001B[1;32m    883\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/mujoco_menagerie-9eYthP1G/lib/python3.12/site-packages/urllib3/response.py:856\u001B[0m, in \u001B[0;36mHTTPResponse._fp_read\u001B[0;34m(self, amt, read1)\u001B[0m\n\u001B[1;32m    853\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1(amt) \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread1()\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# StringIO doesn't like amt=None\u001B[39;00m\n\u001B[0;32m--> 856\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:479\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[0;34m(self, amt)\u001B[0m\n\u001B[1;32m    476\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m amt \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength:\n\u001B[1;32m    477\u001B[0m     \u001B[38;5;66;03m# clip the read to the \"end of response\"\u001B[39;00m\n\u001B[1;32m    478\u001B[0m     amt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength\n\u001B[0;32m--> 479\u001B[0m s \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mamt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m s \u001B[38;5;129;01mand\u001B[39;00m amt:\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[1;32m    482\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[0;32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:720\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    718\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    719\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 720\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    721\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    722\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "IMAGES_URL = r\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n",
    "IMAGES_DIR = \"Images\"\n",
    "IMAGES_TAR_PATH = os.path.join(os.getcwd(), f\"{IMAGES_DIR}.tar\")\n",
    "\n",
    "ANNS_METADATA_URL = r\"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n",
    "ANNS_METADATA = \"keypoint_definitions.csv\"\n",
    "\n",
    "# Download if dataset does not exists.\n",
    "if not os.path.exists(IMAGES_DIR):\n",
    "    download_and_unzip(IMAGES_URL, IMAGES_TAR_PATH)\n",
    "    os.remove(IMAGES_TAR_PATH)\n",
    "\n",
    "if not os.path.isfile(ANNS_METADATA):\n",
    "    download_and_unzip(ANNS_METADATA_URL, ANNS_METADATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd54ce2-5830-4c21-8b61-65f27d5742e7",
   "metadata": {
    "id": "1dd54ce2-5830-4c21-8b61-65f27d5742e7"
   },
   "source": [
    "The annotations downloaded after filling out the form mentioned earlier are maintained in the `StanfordExtra_V12` directory which contains the annotation **JSON** file: `StanfordExtra_v12.json` contain the following structure:\n",
    "\n",
    "```python\n",
    "StanfordExtra_V12\n",
    "├── StanfordExtra_v12.json\n",
    "├── test_stanford_StanfordExtra_v12.npy\n",
    "├── train_stanford_StanfordExtra_v12.npy\n",
    "└── val_stanford_StanfordExtra_v12.npy\n",
    "```\n",
    "\n",
    "The train, validation, and test splits are provided as indices from the original **`StanfordExtra_v12.json`** data.\n",
    "\n",
    "The train, validation, and test sets contain annotations for **6773**, **4062**, and **1703** images, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7177139c-ed62-48db-8f07-583da0546f72",
   "metadata": {
    "id": "7177139c-ed62-48db-8f07-583da0546f72",
    "ExecuteTime": {
     "end_time": "2024-12-04T01:08:21.784190Z",
     "start_time": "2024-12-04T01:08:21.757906Z"
    }
   },
   "outputs": [],
   "source": [
    "ANN_PATH = \"StanfordExtra_V12\"\n",
    "JSON_PATH = os.path.join(ANN_PATH, \"StanfordExtra_v12.json\")\n",
    "\n",
    "with open(JSON_PATH) as file:\n",
    "    json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5258f50-139a-40cf-8172-0a20a0ad7f3d",
   "metadata": {
    "id": "f5258f50-139a-40cf-8172-0a20a0ad7f3d"
   },
   "source": [
    "The files: **`train_stanford_StanfordExtra_v12.npy`** and **`test_stanford_StanfordExtra_v12.npy`** consist of the training and validation indices with respect to the original json_data list.\n",
    "\n",
    "For simplicity, we shall use the test data for validation. The training and the test sets comprise **6773** and **1703** samples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a332c0b6-e228-44f9-878b-081f28d822dd",
   "metadata": {
    "id": "a332c0b6-e228-44f9-878b-081f28d822dd",
    "outputId": "88790fae-05c8-4d0d-dfe2-b066021d85c2",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783306Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ids = np.load(os.path.join(ANN_PATH,\n",
    "                                 \"train_stanford_StanfordExtra_v12.npy\"))\n",
    "val_ids = np.load(os.path.join(ANN_PATH,\n",
    "                               \"test_stanford_StanfordExtra_v12.npy\"))\n",
    "\n",
    "print(f\"Train Samples: {len(train_ids)}\")\n",
    "print(f\"Validation Samples: {len(val_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89f50b-e951-40ab-9f44-68e68f731347",
   "metadata": {
    "id": "ce89f50b-e951-40ab-9f44-68e68f731347"
   },
   "source": [
    "## 3 Create YOLO Train and Valid Directories"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783418Z"
    }
   },
   "id": "a0c116a06c6de965"
  },
  {
   "cell_type": "markdown",
   "id": "012e6114-d72d-44f9-8da5-57fde95db878",
   "metadata": {
    "id": "012e6114-d72d-44f9-8da5-57fde95db878"
   },
   "source": [
    "We will maintain the following directory structure for YOLOv8 dataset:\n",
    "\n",
    "```python\n",
    "animal-pose-data\n",
    "├── train\n",
    "│   ├── images (6773 files)\n",
    "│   └── labels (6773 files)\n",
    "└── valid\n",
    "    ├── images (1703 files)\n",
    "    └── labels (1703 files)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c",
   "metadata": {
    "id": "fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783504Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"animal-pose-data\"\n",
    "\n",
    "TRAIN_DIR         = f\"train\"\n",
    "TRAIN_FOLDER_IMG    = f\"images\"\n",
    "TRAIN_FOLDER_LABELS = f\"labels\"\n",
    "\n",
    "TRAIN_IMG_PATH   = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_IMG)\n",
    "TRAIN_LABEL_PATH = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_LABELS)\n",
    "\n",
    "VALID_DIR           = f\"valid\"\n",
    "VALID_FOLDER_IMG    = f\"images\"\n",
    "VALID_FOLDER_LABELS = f\"labels\"\n",
    "\n",
    "VALID_IMG_PATH   = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_IMG)\n",
    "VALID_LABEL_PATH = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_LABELS)\n",
    "\n",
    "os.makedirs(TRAIN_IMG_PATH, exist_ok=True)\n",
    "os.makedirs(TRAIN_LABEL_PATH, exist_ok=True)\n",
    "os.makedirs(VALID_IMG_PATH, exist_ok=True)\n",
    "os.makedirs(VALID_LABEL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107901d-b191-4e67-b7a2-f89cd7b13a02",
   "metadata": {
    "id": "3107901d-b191-4e67-b7a2-f89cd7b13a02"
   },
   "source": [
    "Next, we will use `train_ids` and `val_ids` to gather the image and annotation data using `json_data` obtained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786dc4a-13bd-407d-9948-317c897068c8",
   "metadata": {
    "id": "3786dc4a-13bd-407d-9948-317c897068c8",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783585Z"
    }
   },
   "outputs": [],
   "source": [
    "train_json_data = []\n",
    "for train_id in train_ids:\n",
    "    train_json_data.append(json_data[train_id])\n",
    "\n",
    "val_json_data = []\n",
    "for val_id in val_ids:\n",
    "    val_json_data.append(json_data[val_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fbe67-9a16-4fed-bcad-80289bb83d2c",
   "metadata": {
    "id": "883fbe67-9a16-4fed-bcad-80289bb83d2c"
   },
   "source": [
    "### 3.1 Copy Image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fd4e1-58a1-4af0-90f3-4392025356a7",
   "metadata": {
    "id": "c82fd4e1-58a1-4af0-90f3-4392025356a7",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783677Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in train_json_data:\n",
    "    img_file = data[\"img_path\"]\n",
    "    filename = img_file.split(\"/\")[-1]\n",
    "    copyfile(os.path.join(IMAGES_DIR, img_file),\n",
    "             os.path.join(TRAIN_IMG_PATH, filename))\n",
    "\n",
    "\n",
    "for data in val_json_data:\n",
    "    img_file = data[\"img_path\"]\n",
    "    filename = img_file.split(\"/\")[-1]\n",
    "    copyfile(os.path.join(IMAGES_DIR, img_file),\n",
    "             os.path.join(VALID_IMG_PATH, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc611e8-9105-4392-823c-4ff12256f5e4",
   "metadata": {
    "id": "3dc611e8-9105-4392-823c-4ff12256f5e4"
   },
   "source": [
    "### 3.2 Create YOLO Annotation TXT FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d036a60-0f0e-4652-8f5d-fd7d101a9f76",
   "metadata": {
    "id": "1d036a60-0f0e-4652-8f5d-fd7d101a9f76"
   },
   "source": [
    "Our final task for data preparation is to create the boxes and the keypoint annotations in accordance with Ultralytics’ YOLO. Since we will deal with a single class (i.e., dogs), we set the class index to **`0`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d75bd-e340-4696-9d9b-97381ae1fd25",
   "metadata": {
    "id": "b11d75bd-e340-4696-9d9b-97381ae1fd25",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783761Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec",
   "metadata": {
    "id": "4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec"
   },
   "source": [
    "The function **`create_yolo_boxes_kpts`** performs the following tasks:\n",
    "\n",
    "* Modifies visibility indicators for keypoints (setting the visibilities for labeled keypoints to 2).\n",
    "* Normalizes the coordinates of both bounding boxes and keypoints relative to the image dimensions.\n",
    "* Converts bounding boxes to $[x_{center}, \\ , y_{center},\\ width,\\ height]$ in normalized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2871243-41d0-4a36-92c1-17d4f250fbcf",
   "metadata": {
    "id": "c2871243-41d0-4a36-92c1-17d4f250fbcf",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.783978Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_yolo_boxes_kpts(img_size, boxes, lm_kpts):\n",
    "\n",
    "    IMG_W, IMG_H = img_size\n",
    "    # Modify kpts with visibilities as 1s to 2s.\n",
    "    vis_ones = np.where(lm_kpts[:, -1] == 1.)\n",
    "    lm_kpts[vis_ones, -1] = 2.\n",
    "\n",
    "    # Normalizing factor for bboxes and kpts.\n",
    "    res_box_array = np.array([IMG_W, IMG_H, IMG_W, IMG_H])\n",
    "    res_lm_array = np.array([IMG_W, IMG_H])\n",
    "\n",
    "    # Normalize landmarks in the range [0,1].\n",
    "    norm_kps_per_img = lm_kpts.copy()\n",
    "    norm_kps_per_img[:, :-1]  = norm_kps_per_img[:, :-1] / res_lm_array\n",
    "\n",
    "    # Normalize bboxes in the range [0,1].\n",
    "    norm_bbox_per_img = boxes / res_box_array\n",
    "\n",
    "    # Create bboxes coordinates to YOLO.\n",
    "    # x_c, y_c = x_min + bbox_w/2. , y_min + bbox_h/2.\n",
    "    yolo_boxes = norm_bbox_per_img.copy()\n",
    "    yolo_boxes[:2] = norm_bbox_per_img[:2] + norm_bbox_per_img[2:]/2.\n",
    "\n",
    "    return yolo_boxes, norm_kps_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4abe0-3467-4575-97d6-51e4c1226b73",
   "metadata": {
    "id": "fda4abe0-3467-4575-97d6-51e4c1226b73"
   },
   "source": [
    "We will finally create the `txt` files for YOLO based on the `train_json_data` and `val_json_data` obtained earlier. The function `create_yolo_txt_files` creates the required `txt` annotations in YOLO using the `create_yolo_boxes_kpts` utility function explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fa0ee-d9df-4d4f-8493-012a39959bcb",
   "metadata": {
    "id": "6f7fa0ee-d9df-4d4f-8493-012a39959bcb",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784114Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_yolo_txt_files(json_data, LABEL_PATH):\n",
    "\n",
    "    for data in json_data:\n",
    "\n",
    "        IMAGE_ID = data[\"img_path\"].split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        IMG_WIDTH, IMG_HEIGHT = data[\"img_width\"], data[\"img_height\"]\n",
    "\n",
    "        landmark_kpts  = np.nan_to_num(np.array(data[\"joints\"], dtype=np.float32))\n",
    "        landmarks_bboxes = np.array(data[\"img_bbox\"], dtype=np.float32)\n",
    "\n",
    "        bboxes_yolo, kpts_yolo = create_yolo_boxes_kpts(\n",
    "                                            (IMG_WIDTH, IMG_HEIGHT),\n",
    "                                            landmarks_bboxes,\n",
    "                                            landmark_kpts)\n",
    "\n",
    "        TXT_FILE = IMAGE_ID+\".txt\"\n",
    "\n",
    "        with open(os.path.join(LABEL_PATH, TXT_FILE), \"w\") as f:\n",
    "\n",
    "            x_c_norm, y_c_norm, box_width_norm, box_height_norm = round(bboxes_yolo[0],5),\\\n",
    "                                                                  round(bboxes_yolo[1],5),\\\n",
    "                                                                  round(bboxes_yolo[2],5),\\\n",
    "                                                                  round(bboxes_yolo[3],5),\\\n",
    "\n",
    "            kps_flattend = [round(ele,5) for ele in kpts_yolo.flatten().tolist()]\n",
    "            line = f\"{CLASS_ID} {x_c_norm} {y_c_norm} {box_width_norm} {box_height_norm} \"\n",
    "            line+= \" \".join(map(str, kps_flattend))\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbb9989-e536-45d5-b198-771a16ce3025",
   "metadata": {
    "id": "9bbb9989-e536-45d5-b198-771a16ce3025"
   },
   "source": [
    "Finnally, we create the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6a84a-5678-475d-b95a-3b2b6ccba51c",
   "metadata": {
    "id": "a0c6a84a-5678-475d-b95a-3b2b6ccba51c",
    "ExecuteTime": {
     "end_time": "2024-12-04T01:08:21.958812Z",
     "start_time": "2024-12-04T01:08:21.784204Z"
    }
   },
   "outputs": [],
   "source": [
    "create_yolo_txt_files(train_json_data, TRAIN_LABEL_PATH)\n",
    "create_yolo_txt_files(val_json_data, VALID_LABEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98afac-c5cf-4347-985c-6286362316dd",
   "metadata": {
    "id": "aa98afac-c5cf-4347-985c-6286362316dd"
   },
   "source": [
    "## 4 Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f3b5e-5554-4309-8992-0ef691aa93ee",
   "metadata": {
    "id": "010f3b5e-5554-4309-8992-0ef691aa93ee"
   },
   "source": [
    "Before visualizing the samples, we can map the `hexadecimal` color codings available with **`keypoint_definitions.csv`** to RGB values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb68085-e8ac-49d2-a319-83f6d0fad95a",
   "metadata": {
    "id": "feb68085-e8ac-49d2-a319-83f6d0fad95a",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784412Z"
    }
   },
   "outputs": [],
   "source": [
    "ann_meta_data = pd.read_csv(\"keypoint_definitions.csv\")\n",
    "COLORS = ann_meta_data[\"Hex colour\"].values.tolist()\n",
    "\n",
    "COLORS_RGB_MAP = []\n",
    "for color in COLORS:\n",
    "    R, G, B = int(color[:2], 16), int(color[2:4], 16), int(color[4:], 16)\n",
    "    COLORS_RGB_MAP.append({color: (R,G,B)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605d8a97-a77b-4e37-a51f-dff797ec73ab",
   "metadata": {
    "id": "605d8a97-a77b-4e37-a51f-dff797ec73ab",
    "outputId": "5ed09a10-9f7e-413c-e36e-820f8038b5ac",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784531Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images = os.listdir(TRAIN_IMG_PATH)\n",
    "valid_images = os.listdir(VALID_IMG_PATH)\n",
    "\n",
    "print(f\"Training images: {len(train_images)}, Validation Images: {len(valid_images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b31f59-c466-42a7-87a6-68a0c9a0c82f",
   "metadata": {
    "id": "78b31f59-c466-42a7-87a6-68a0c9a0c82f"
   },
   "source": [
    "The `draw_landmarks` function is used to annotate the corresponding landmark points on the image using COLORS_RGB_MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e7df6-4a58-4e85-ab41-ebd533278f8e",
   "metadata": {
    "id": "f99e7df6-4a58-4e85-ab41-ebd533278f8e",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784609Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_landmarks(image, landmarks):\n",
    "\n",
    "    radius = 5\n",
    "    # Check if image width is greater than 1000 px.\n",
    "    # To improve visualization.\n",
    "    if (image.shape[1] > 1000):\n",
    "        radius = 8\n",
    "\n",
    "    for idx, kpt_data in enumerate(landmarks):\n",
    "\n",
    "        loc_x, loc_y = kpt_data[:2].astype(\"int\").tolist()\n",
    "        color_id = list(COLORS_RGB_MAP[int(kpt_data[-1])].values())[0]\n",
    "\n",
    "        cv2.circle(image,\n",
    "                   (loc_x, loc_y),\n",
    "                   radius,\n",
    "                   color=color_id[::-1],\n",
    "                   thickness=-1,\n",
    "                   lineType=cv2.LINE_AA)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4535983-422b-480e-bb3d-2a79dd028eca",
   "metadata": {
    "id": "e4535983-422b-480e-bb3d-2a79dd028eca"
   },
   "source": [
    "The `draw_boxes` function is used to annotate the bounding boxes along with the confidence scores (if passed) on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb17095-84bb-499e-aea4-4fc9c4e67d0f",
   "metadata": {
    "id": "7cb17095-84bb-499e-aea4-4fc9c4e67d0f",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784739Z"
    }
   },
   "outputs": [],
   "source": [
    "def draw_boxes(image, detections, class_name = \"dog\", score=None, color=(0,255,0)):\n",
    "\n",
    "    font_size = 0.25 + 0.07 * min(image.shape[:2]) / 100\n",
    "    font_size = max(font_size, 0.5)\n",
    "    font_size = min(font_size, 0.8)\n",
    "    text_offset = 3\n",
    "\n",
    "    thickness = 2\n",
    "    # Check if image width is greater than 1000 px.\n",
    "    # To improve visualization.\n",
    "    if (image.shape[1] > 1000):\n",
    "        thickness = 10\n",
    "\n",
    "    xmin, ymin, xmax, ymax = detections[:4].astype(\"int\").tolist()\n",
    "    conf = round(float(detections[-1]),2)\n",
    "    cv2.rectangle(image,\n",
    "                  (xmin, ymin),\n",
    "                  (xmax, ymax),\n",
    "                  color=(0,255,0),\n",
    "                  thickness=thickness,\n",
    "                  lineType=cv2.LINE_AA)\n",
    "\n",
    "    display_text = f\"{class_name}\"\n",
    "\n",
    "    if score is not None:\n",
    "        display_text+=f\": {score:.2f}\"\n",
    "\n",
    "    (text_width, text_height), _ = cv2.getTextSize(display_text,\n",
    "                                                   cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                                   font_size, 2)\n",
    "\n",
    "    cv2.rectangle(image,\n",
    "                      (xmin, ymin),\n",
    "                      (xmin + text_width + text_offset, ymin - text_height - int(15 * font_size)),\n",
    "                      color=color, thickness=-1)\n",
    "\n",
    "    image = cv2.putText(\n",
    "                    image,\n",
    "                    display_text,\n",
    "                    (xmin + text_offset, ymin - int(10 * font_size)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_size,\n",
    "                    (0, 0, 0),\n",
    "                    2, lineType=cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a9245-e884-4dec-b667-168717117b9e",
   "metadata": {
    "id": "283a9245-e884-4dec-b667-168717117b9e"
   },
   "source": [
    "The `visualize_annotations` is used to annotate both the bounding box coordinates and the landmark keypoints on the corresponding image after converting them to absoulute coordinates.\n",
    "\n",
    "Recall that both the bounding box coordinates and the keypoints were normalized in the range `[0, 1]`. However, to plot them, we need the absolute coordinates.\n",
    "\n",
    "The conversion mapping from YOLO bboxes to $[x_{min}, y_{min}, x_{max}, y_{max}]$ is pretty straight forward and can be obtained using the following set of equations:\n",
    "\n",
    "$$x_{min} = \\frac{W}{2} (2x_{center} \\ - \\ width)$$\n",
    "\n",
    "$$y_{min} = \\frac{H}{2} (2y_{center} \\ - \\ height)$$\n",
    "\n",
    "$$x_{max} = x_{min} + width * W$$\n",
    "\n",
    "$$y_{max} = y_{min} + height * H$$\n",
    "\n",
    "\n",
    "Similarly, the keypoints can denormalized (to the absolute coordinates) using:\n",
    "\n",
    "$$x_{abs} = x_{norm}* W$$\n",
    "\n",
    "$$y_{abs} = y_{norm}* H$$\n",
    "\n",
    "\n",
    "Here, the `width` and `height` are the box width and height respectively; whereas `W` and `H` are the image width and height respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a22ae9-59bc-468e-8217-0c5cc67a75dc",
   "metadata": {
    "id": "d2a22ae9-59bc-468e-8217-0c5cc67a75dc",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784797Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_annotations(image, box_data, keypoints_data):\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    shape_multiplier = np.array(image.shape[:2][::-1]) # (W, H).\n",
    "    # Final absolute coordinates (xmin, ymin, xmax, ymax).\n",
    "    denorm_boxes = np.zeros_like(box_data)\n",
    "\n",
    "    # De-normalize center coordinates from YOLO to (xmin, ymin).\n",
    "    denorm_boxes[:, :2] = (shape_multiplier/2.) * (2*box_data[:,:2] - box_data[:,2:])\n",
    "\n",
    "    # De-normalize width and height from YOLO to (xmax, ymax).\n",
    "    denorm_boxes[:, 2:] = denorm_boxes[:,:2] + box_data[:,2:]*shape_multiplier\n",
    "\n",
    "    for boxes, kpts in zip(denorm_boxes, keypoints_data):\n",
    "        # De-normalize landmark coordinates.\n",
    "        kpts[:, :2]*= shape_multiplier\n",
    "        image = draw_boxes(image, boxes)\n",
    "        image = draw_landmarks(image, kpts)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084158f-5e58-419b-8cfc-40fab86839ae",
   "metadata": {
    "id": "b084158f-5e58-419b-8cfc-40fab86839ae"
   },
   "source": [
    "The following plot shows a few image samples with their corresponding ground truth annotation. The keypoint annotations are filtered based on their corresponding visibility flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326aa95f-33d7-43c3-868a-826324258898",
   "metadata": {
    "id": "326aa95f-33d7-43c3-868a-826324258898",
    "outputId": "ee5e9f1d-9cc2-47d1-9129-947ce58033a6",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784859Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_FILES = os.listdir(TRAIN_IMG_PATH)\n",
    "NUM_LANDMARKS = 24\n",
    "\n",
    "num_samples = 8\n",
    "num_rows = 2\n",
    "num_cols = num_samples//num_rows\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=num_cols,\n",
    "        figsize=(25, 15),\n",
    "    )\n",
    "\n",
    "random.seed(45)\n",
    "random.shuffle(IMAGE_FILES)\n",
    "\n",
    "for idx, (file, axis) in enumerate(zip(IMAGE_FILES[:num_samples], ax.flat)):\n",
    "\n",
    "    image = cv2.imread(os.path.join(TRAIN_IMG_PATH, file))\n",
    "\n",
    "    # Obtain the txt file for the corresponding image file.\n",
    "    filename = file.split(\".\")[0]\n",
    "    # Split each object instance in separate lists.\n",
    "    with open(os.path.join(TRAIN_LABEL_PATH, filename+\".txt\"), \"r\") as file:\n",
    "        label_data = [x.split() for x in file.read().strip().splitlines() if len(x)]\n",
    "\n",
    "    label_data = np.array(label_data, dtype=np.float32)\n",
    "\n",
    "    # YOLO BBox instances in [x-center, y-center, width, height] in normalized form.\n",
    "    box_instances = label_data[:,1:5]\n",
    "    # Shape: (N, 4), where, N = #instances per-image\n",
    "\n",
    "    # Kpt instances.\n",
    "    # Filter keypoints based on visibility.\n",
    "    instance_kpts = []\n",
    "    kpts_data = label_data[:,5:].reshape(-1, NUM_LANDMARKS, 3)\n",
    "\n",
    "    for inst_kpt in kpts_data:\n",
    "        vis_ids = np.where(inst_kpt[:, -1]>0.)[0]\n",
    "        vis_kpts = inst_kpt[vis_ids][:,:2]\n",
    "        vis_kpts = np.concatenate([vis_kpts, np.expand_dims(vis_ids, axis=-1)], axis=-1)\n",
    "        instance_kpts.append(vis_kpts)\n",
    "\n",
    "    image_ann = visualize_annotations(image, box_instances, instance_kpts)\n",
    "    axis.imshow(image_ann[...,::-1])\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.tight_layout(h_pad=4., w_pad=4.)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a171a6-8a1d-4215-bbdf-a0c0bdde5c36",
   "metadata": {
    "id": "98a171a6-8a1d-4215-bbdf-a0c0bdde5c36"
   },
   "source": [
    "## 5 Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5071680-68cc-4240-9ef5-e6fc609ba156",
   "metadata": {
    "id": "e5071680-68cc-4240-9ef5-e6fc609ba156"
   },
   "source": [
    "### 5.1 Training Configuration\n",
    "\n",
    "We shall define the training configuration for fine-tuning in the `TrainingConfig` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e754d5c-1be0-4238-b113-3a02ad8f25a6",
   "metadata": {
    "id": "7e754d5c-1be0-4238-b113-3a02ad8f25a6",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784918Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    DATASET_YAML:   str = \"animal-keypoints.yaml\"\n",
    "    MODEL:          str = \"yolov8m-pose.pt\"\n",
    "    EPOCHS:         int = 100\n",
    "    KPT_SHAPE:    tuple = (24,3)\n",
    "    PROJECT:        str = \"Animal_Keypoints\"\n",
    "    NAME:           str = f\"{MODEL.split('.')[0]}_{EPOCHS}_epochs\"\n",
    "    CLASSES_DICT:  dict = field(default_factory = lambda:{0 : \"dog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c05db-5e44-4574-9a23-e8fc00e7b099",
   "metadata": {
    "id": "120c05db-5e44-4574-9a23-e8fc00e7b099"
   },
   "source": [
    "### 5.2 Data Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473ee95-d50d-4cf3-b2de-e63d75b03895",
   "metadata": {
    "id": "3473ee95-d50d-4cf3-b2de-e63d75b03895"
   },
   "source": [
    "The `DatasetConfig` class takes in the various hyperparameters related to the data such as the image size and batch size to be used while training, along with the various augmentation probabilities such as Mosaic, horizontal flip, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769ab0c-7773-4fe4-a828-76ec17d3cff8",
   "metadata": {
    "id": "1769ab0c-7773-4fe4-a828-76ec17d3cff8",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.784973Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DatasetConfig:\n",
    "    IMAGE_SIZE:    int   = 640\n",
    "    BATCH_SIZE:    int   = 16\n",
    "    CLOSE_MOSAIC:  int   = 10\n",
    "    MOSAIC:        float = 0.4\n",
    "    FLIP_LR:       float = 0.0 # Turn off horizontal flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005212b5-80e9-4487-9f1f-3c9ec2de6fcc",
   "metadata": {
    "id": "005212b5-80e9-4487-9f1f-3c9ec2de6fcc",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.785031Z"
    }
   },
   "outputs": [],
   "source": [
    "train_config = TrainingConfig()\n",
    "data_config = DatasetConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be43529c-85a0-4bb7-86e4-463bfab82d54",
   "metadata": {
    "id": "be43529c-85a0-4bb7-86e4-463bfab82d54"
   },
   "source": [
    "Before we start our training, we need to create a `yaml` containing the path to the images and label files. We also need to specify the class names, starting from index=0 and the keypoint shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c63d59-91b9-455a-92cc-355af5aa0dc5",
   "metadata": {
    "id": "77c63d59-91b9-455a-92cc-355af5aa0dc5",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.785080Z"
    }
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "\n",
    "data_dict = dict(\n",
    "                path      = os.path.join(current_dir, DATA_DIR),\n",
    "                train     = os.path.join(TRAIN_DIR, TRAIN_FOLDER_IMG),\n",
    "                val       = os.path.join(VALID_DIR, VALID_FOLDER_IMG),\n",
    "                names     = train_config.CLASSES_DICT,\n",
    "                kpt_shape = list(train_config.KPT_SHAPE),\n",
    "               )\n",
    "\n",
    "with open(train_config.DATASET_YAML, \"w\") as config_file:\n",
    "    yaml.dump(data_dict, config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7c435-78fb-4d76-aea0-335da96db326",
   "metadata": {
    "id": "13a7c435-78fb-4d76-aea0-335da96db326"
   },
   "source": [
    "## 6 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fffd85-cccc-446e-b51a-b885991a2b6f",
   "metadata": {
    "id": "54fffd85-cccc-446e-b51a-b885991a2b6f",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.785870Z"
    }
   },
   "outputs": [],
   "source": [
    "pose_model = model = YOLO(train_config.MODEL)\n",
    "\n",
    "pose_model.train(data    = train_config.DATASET_YAML,\n",
    "            epochs       = train_config.EPOCHS,\n",
    "            imgsz        = data_config.IMAGE_SIZE,\n",
    "            batch        = data_config.BATCH_SIZE,\n",
    "            project      = train_config.PROJECT,\n",
    "            name         = train_config.NAME,\n",
    "            close_mosaic = data_config.CLOSE_MOSAIC,\n",
    "            mosaic       = data_config.MOSAIC,\n",
    "            fliplr       = data_config.FLIP_LR\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e1adb-eaae-44f1-880a-e7544e7b6f6a",
   "metadata": {
    "id": "6d9e1adb-eaae-44f1-880a-e7544e7b6f6a"
   },
   "source": [
    "## 7 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506e12a5-c16d-401a-98e8-f1cd45ac05f1",
   "metadata": {
    "id": "506e12a5-c16d-401a-98e8-f1cd45ac05f1",
    "outputId": "86d39f67-7499-4422-e547-2d316c8a23d9",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.785970Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_path  = os.path.join(train_config.PROJECT, train_config.NAME, \"weights\", \"best.pt\")\n",
    "model_pose = YOLO(ckpt_path)\n",
    "\n",
    "metrics = model_pose.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f18c87-bb0c-45f6-a499-0a2086073fe6",
   "metadata": {
    "id": "b8f18c87-bb0c-45f6-a499-0a2086073fe6"
   },
   "source": [
    "## 8 Predictions\n",
    "\n",
    "The `prepare_predictions` function obtains the predicted boxes, confidence scores, and keypoints for the corresponding image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f8f3b-263e-443b-944a-057b935c188a",
   "metadata": {
    "id": "802f8f3b-263e-443b-944a-057b935c188a",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.786030Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_predictions(\n",
    "    image_dir_path,\n",
    "    image_filename,\n",
    "    model,\n",
    "    BOX_IOU_THRESH = 0.55,\n",
    "    BOX_CONF_THRESH=0.30,\n",
    "    KPT_CONF_THRESH=0.68):\n",
    "\n",
    "    image_path = os.path.join(image_dir_path, image_filename)\n",
    "    image = cv2.imread(image_path).copy()\n",
    "\n",
    "    results = model.predict(image_path, conf=BOX_CONF_THRESH, iou=BOX_IOU_THRESH)[0].cpu()\n",
    "\n",
    "    if not len(results.boxes.xyxy):\n",
    "        return image\n",
    "\n",
    "    # Get the predicted boxes, conf scores and keypoints.\n",
    "    pred_boxes = results.boxes.xyxy.numpy()\n",
    "    pred_box_conf = results.boxes.conf.numpy()\n",
    "    pred_kpts_xy = results.keypoints.xy.numpy()\n",
    "    pred_kpts_conf = results.keypoints.conf.numpy()\n",
    "\n",
    "    # Draw predicted bounding boxes, conf scores and keypoints on image.\n",
    "    for boxes, score, kpts, confs in zip(pred_boxes, pred_box_conf, pred_kpts_xy, pred_kpts_conf):\n",
    "        kpts_ids = np.where(confs > KPT_CONF_THRESH)[0]\n",
    "        filter_kpts = kpts[kpts_ids]\n",
    "        filter_kpts = np.concatenate([filter_kpts, np.expand_dims(kpts_ids, axis=-1)], axis=-1)\n",
    "        image = draw_boxes(image, boxes, score=score)\n",
    "        image = draw_landmarks(image, filter_kpts)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7de57-709d-4ae3-8345-f6673b4d34d0",
   "metadata": {
    "id": "97c7de57-709d-4ae3-8345-f6673b4d34d0",
    "outputId": "3d000f70-c331-4cd0-b270-6154162d0b1c",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.786109Z"
    }
   },
   "outputs": [],
   "source": [
    "VAL_IMAGE_FILES = os.listdir(VALID_IMG_PATH)\n",
    "\n",
    "num_samples = 9\n",
    "num_rows = 3\n",
    "num_cols = num_samples//num_rows\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "        nrows=num_rows,\n",
    "        ncols=num_cols,\n",
    "        figsize=(25, 15),\n",
    "    )\n",
    "\n",
    "random.seed(90)\n",
    "random.shuffle(VAL_IMAGE_FILES)\n",
    "\n",
    "for idx, (file, axis) in enumerate(zip(VAL_IMAGE_FILES[:num_samples], ax.flat)):\n",
    "\n",
    "    image_pred = prepare_predictions(VALID_IMG_PATH, file, model_pose)\n",
    "    axis.imshow(image_pred[...,::-1])\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "plt.tight_layout(h_pad=4., w_pad=4.)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2eeae",
   "metadata": {
    "id": "38b2eeae",
    "ExecuteTime": {
     "start_time": "2024-12-04T01:08:21.786176Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
